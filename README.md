# âš¡ EnergyZero ETL Pipeline

## ğŸ§© Project Overview
This project fetches energy price data from the **EnergyZero API**, processes it with **Apache Airflow**, and converts it to **Parquet** format. All components run in **Docker**, making the pipeline portable and reproducible.

## ğŸš€ Project Structure
energyzero_etl/
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ dags/
â”‚ â””â”€â”€ energyzero_etl.py # Airflow DAG
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ extract_energyzero.py # Fetch JSON from API
â”‚ â””â”€â”€ transform_pandas.py # Transform JSON to Parquet
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Raw JSON files
â”‚ â””â”€â”€ processed/ # Transformed Parquet files
â””â”€â”€ README.md

markdown
Kodu kopyala

## âš™ï¸ Running the Project
1. **Start Docker Environment**  
```bash
docker-compose up -d
This launches:

Airflow webserver & scheduler

PostgreSQL database

Airflow UI: http://localhost:8080
Username: airflow | Password: airflow

Run the DAG
Turn on the energyzero_etl DAG in the Airflow UI and trigger it manually or wait for the schedule.

Outputs

Raw data: data/raw/

Processed data: data/processed/energy_transformed.parquet

ğŸ§  Key Learnings
Build multi-component ETL pipelines in Docker

Schedule tasks using Apache Airflow

Transform JSON to Parquet using Pandas

ğŸ’¡ Next Steps / Improvements
Load Parquet into PostgreSQL or DuckDB

Add logging or error notifications to Airflow DAG

Extend API date ranges

Create visualization scripts with Matplotlib or Plotly

ğŸ‘¨â€ğŸ’» Developer
Mustapha Haybat | Netherlands
ğŸ”— LinkedIn Profile https://www.linkedin.com/in/mustafa-haybat-86725235a/
